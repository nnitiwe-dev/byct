{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### # AI Workshop: Building a Smart Hiring System with RAG-Powered Applicant Tracking\n",
        "\n",
        "#### Introduction\n",
        "\n",
        "Traditional Applicant Tracking Systems (ATS) often struggle with handling large volumes of data, maintaining context, and applying insights to niche domains, leading to inefficiencies and inaccuracies.\n",
        "\n",
        "This is where Retrieval-Augmented Generation (RAG) comes into play, addressing key issues like hallucination in AI responses and providing domain-specific context.\n",
        "\n",
        "#### What is RAG?\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a cutting-edge approach that combines the strengths of information retrieval and text generation models. Unlike standard language models that generate responses based solely on their training data, RAG retrieves relevant information from external datasets, ensuring that the generated output is accurate, contextually relevant, and grounded in real data. This method significantly reduces the risk of hallucination, where AI generates plausible but incorrect information, and enhances the system's ability to apply insights to niche domains.\n",
        "\n",
        "#### Benefits to Human Resource Departments\n",
        "\n",
        "Implementing a RAG-powered Applicant Tracking System (ATS) offers numerous benefits to HR departments:\n",
        "\n",
        "- **Enhanced Accuracy**: By retrieving and utilizing real-time data, RAG ensures that the AI-generated insights and recommendations are accurate and relevant.\n",
        "- **Contextual Relevance**: RAG provides contextual answers tailored to specific job roles and industry requirements, improving the quality of candidate evaluations.\n",
        "- **Efficiency**: Automating the retrieval and analysis of applicant data saves time and reduces the workload on HR personnel, allowing them to focus on strategic decision-making.\n",
        "- **Scalability**: RAG-powered systems can handle large volumes of applications, ensuring consistent and fair candidate assessments.\n",
        "\n",
        "#### About the Dataset\n",
        "\n",
        "The dataset used in this workshop is sourced from Kaggle and can be accessed [here](https://www.kaggle.com/datasets/shivani12sharma/resume-dataset-new/data). It contains a comprehensive collection of resumes across various job roles and industries, providing a rich source of information for training and testing our RAG-powered ATS. The dataset includes details such as candidate skills, experiences, education, and other relevant attributes, enabling us to build a robust system capable of nuanced and context-aware applicant tracking and evaluation.\n"
      ],
      "metadata": {
        "id": "rCmbG5FD99yP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjWpSvpJ_uPs",
        "outputId": "ec59230f-7b0e-4175-d1eb-8c59f0e484a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/resume/archive_resume.zip\n",
            "  inflating: 1.pdf                   \n",
            "  inflating: 10.pdf                  \n",
            "  inflating: 11.pdf                  \n",
            "  inflating: 12.pdf                  \n",
            "  inflating: 13.pdf                  \n",
            "  inflating: 14.pdf                  \n",
            "  inflating: 15.pdf                  \n",
            "  inflating: 16.pdf                  \n",
            "  inflating: 17.pdf                  \n",
            "  inflating: 18.pdf                  \n",
            "  inflating: 19.pdf                  \n",
            "  inflating: 2.pdf                   \n",
            "  inflating: 20.pdf                  \n",
            "  inflating: 21.docx                 \n",
            "  inflating: 22.docx                 \n",
            "  inflating: 23.docx                 \n",
            "  inflating: 24.png                  \n",
            "  inflating: 25.jfif                 \n",
            "  inflating: 26.jfif                 \n",
            "  inflating: 27.jpg                  \n",
            "  inflating: 28.jpeg                 \n",
            "  inflating: 29.jfif                 \n",
            "  inflating: 3.pdf                   \n",
            "  inflating: 4.pdf                   \n",
            "  inflating: 5.pdf                   \n",
            "  inflating: 6.pdf                   \n",
            "  inflating: 7.pdf                   \n",
            "  inflating: 8.pdf                   \n",
            "  inflating: 9.pdf                   \n"
          ]
        }
      ],
      "source": [
        "#@title Download Resume Dataset (Already saved to Google Drive)\n",
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/resume/archive_resume.zip\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What you need to Build System\n",
        "\n",
        "1. A NoSql Vector Database - MongoDB\n",
        "2. Data - Dataset of Resume Files (PDFs)\n",
        "3. Large Language Model - OpenAI\n",
        "\n",
        "\n",
        "## Set Mongo DB\n",
        "\n",
        "Setting up a free MongoDB Atlas database is straightforward. Follow these steps to create an account, set up a cluster, and connect to your database:\n",
        "\n",
        "### 1. **Create a MongoDB Atlas Account**\n",
        "1. Go to the [MongoDB Atlas website](https://www.mongodb.com/cloud/atlas/register).\n",
        "2. Fill out the registration form or use an existing Google account to sign up.\n",
        "3. Verify your email address if prompted.\n",
        "\n",
        "### 2. **Create a New Project**\n",
        "1. Once logged in, click on **\"New Project\"**.\n",
        "2. Enter a name for your project (e.g., \"MyFirstProject\").\n",
        "3. Optionally, add members and set permissions.\n",
        "4. Click **\"Create Project\"**.\n",
        "\n",
        "### 3. **Build a Cluster**\n",
        "1. In your new project, click **\"Build a Cluster\"**.\n",
        "2. Select the **\"Shared Clusters\"** tab to choose a free tier cluster.\n",
        "3. Choose a cloud provider and region. The free tier offers multiple options.\n",
        "4. Customize your cluster settings (or leave defaults) and click **\"Create Cluster\"**.\n",
        "\n",
        "### 4. **Configure Network Access**\n",
        "1. While the cluster is being created, set up network access.\n",
        "2. Go to **\"Network Access\"** in the left-hand menu.\n",
        "3. Click **\"Add IP Address\"**.\n",
        "4. You can add your current IP address or allow access from anywhere by adding `0.0.0.0/0`. Note that allowing access from anywhere can be insecure.\n",
        "\n",
        "### 5. **Create a Database User**\n",
        "1. Go to **\"Database Access\"** in the left-hand menu.\n",
        "2. Click **\"Add New Database User\"**.\n",
        "3. Enter a username and password for your database user.\n",
        "4. Assign roles (e.g., \"Atlas Admin\").\n",
        "5. Click **\"Add User\"**.\n",
        "\n",
        "### 6. **Connect to Your Cluster**\n",
        "1. Once your cluster is created (it may take a few minutes), click **\"Clusters\"** in the left-hand menu.\n",
        "2. Click **\"Connect\"** next to your cluster.\n",
        "3. Choose a connection method:\n",
        "    - **Connect Your Application**: Get the connection string to use in your application code.\n",
        "    - **MongoDB Compass**: Use MongoDB's graphical user interface.\n",
        "    - **Connect from Mongo Shell**: Use the Mongo shell to connect directly.\n",
        "\n",
        "### 7. **Connecting with pymongo in Python**\n",
        "If you choose to connect your application, follow these steps to connect using `pymongo`:\n",
        "\n",
        "1. **Install pymongo**:\n",
        "    ```bash\n",
        "    pip install pymongo\n",
        "    ```\n",
        "\n",
        "2. **Get the Connection String**:\n",
        "    - Select **\"Connect Your Application\"**.\n",
        "    - Choose your driver and version (e.g., Python and 3.6 or later).\n",
        "    - Copy the provided connection string.\n",
        "\n",
        "3. **Use the Connection String in Your Code**:\n",
        "    Replace `<username>`, `<password>`, and `<dbname>` with your database user credentials and database name.\n",
        "\n",
        "    ```python\n",
        "    from pymongo import MongoClient\n",
        "\n",
        "    # Replace <password> with the password for the <username> user.\n",
        "    # Replace <dbname> with the name of the database that connections will use by default.\n",
        "    connection_string = \"mongodb+srv://<username>:<password>@<cluster-url>/<dbname>?retryWrites=true&w=majority\"\n",
        "\n",
        "    client = MongoClient(connection_string)\n",
        "\n",
        "    # Access a database\n",
        "    db = client.get_database('<dbname>')\n",
        "\n",
        "    # Access a collection\n",
        "    collection = db.get_collection('my_collection')\n",
        "\n",
        "    # Perform operations\n",
        "    document = collection.find_one()\n",
        "    print(document)\n",
        "    ```\n",
        "\n",
        "By following these steps, you'll set up a free MongoDB Atlas cluster, configure access, create a database user, and connect to the cluster using Python and `pymongo`.\n",
        "\n"
      ],
      "metadata": {
        "id": "f6UI6NomcYOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl3M_SxMAJoa",
        "outputId": "dfe791bf-2f68-4007-c9af-6bf30449ac69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.4/466.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSelecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123586 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.9-py3-none-any.whl (987 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.7/987.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.20 (from langchain)\n",
            "  Downloading langchain_core-0.2.21-py3-none-any.whl (372 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.0/372.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.90-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.7/134.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.20->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.20->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, tiktoken, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.9 langchain-core-0.2.21 langchain-text-splitters-0.2.2 langsmith-0.1.90 orjson-3.10.6 tiktoken-0.7.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.6/328.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Install all Library Dependencies\n",
        "!pip install \"unstructured[all-docs]\" -q\n",
        "!apt-get -qq install poppler-utils tesseract-ocr -q\n",
        "!pip install -q --user --upgrade pillow -q\n",
        "!pip install langchain tiktoken langchain-community\n",
        "!pip install pymongo==4.6.1 -q\n",
        "!pip install openai -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step-by-Step Guide to Setting Up a Search Index on MongoDB Atlas\n",
        "\n",
        "#### 1. Log In to Your MongoDB Atlas Account\n",
        "- Navigate to the [MongoDB Atlas website](https://cloud.mongodb.com/) and log in with your credentials.\n",
        "\n",
        "#### 2. Select Your Cluster\n",
        "- From the MongoDB Atlas dashboard, select the cluster where you want to create the search index.\n",
        "\n",
        "#### 3. Navigate to the Collections\n",
        "- Click on the **\"Collections\"** tab for your selected cluster to access the database and collections.\n",
        "\n",
        "#### 4. Select Your Database and Collection\n",
        "- Choose the database and the specific collection on which you want to create the search index.\n",
        "\n",
        "#### 5. Create a Search Index\n",
        "- In the collection view, click on the **\"Indexes\"** tab.\n",
        "- Click on the **\"Create Search Index\"** button.\n",
        "\n",
        "#### 6. Configure the Search Index\n",
        "- **Set the Name of the Index**:\n",
        "  - Set the name of the index to `default`.\n",
        "- **Copy and Paste the JSON Configuration**:\n",
        "  - Copy the following JSON configuration into the index configuration window:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mappings\": {\n",
        "    \"dynamic\": false,\n",
        "    \"fields\": {\n",
        "      \"embedding\": {\n",
        "        \"type\": \"knnVector\",\n",
        "        \"dimensions\": 1536,\n",
        "        \"similarity\": \"cosine\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### 7. Explanation of the JSON Configuration\n",
        "- **dynamic: false**: This setting disables dynamic mapping, meaning only fields explicitly defined in the mappings section will be indexed.\n",
        "- **fields.embedding**:\n",
        "  - **type: \"knnVector\"**: Specifies that the embedding field is a vector field that will be used for k-nearest neighbors (kNN) search.\n",
        "  - **dimensions: 1536**: Defines the number of dimensions in the vector.\n",
        "  - **similarity: \"cosine\"**: Sets the similarity metric to cosine similarity.\n",
        "\n",
        "#### 8. Example Configuration\n",
        "- Here is the complete example of how the configuration looks when creating the index:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mappings\": {\n",
        "    \"dynamic\": false,\n",
        "    \"fields\": {\n",
        "      \"embedding\": {\n",
        "        \"type\": \"knnVector\",\n",
        "        \"dimensions\": 1536,\n",
        "        \"similarity\": \"cosine\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### 9. Final Steps\n",
        "- **Create the Index**:\n",
        "  - After pasting the JSON configuration, click on the **\"Create Index\"** button to create the search index.\n",
        "- **Verify the Index**:\n",
        "  - Once the index creation process is complete, verify that the index is listed under the **\"Indexes\"** tab with the name `default`.\n",
        "\n",
        "---\n",
        "\n",
        "By following these steps, you will successfully set up a search index on MongoDB Atlas, allowing you to perform efficient searches on vector fields using k-nearest neighbors (kNN) and cosine similarity."
      ],
      "metadata": {
        "id": "VeCE-T1LGtah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries\n",
        "from pymongo import MongoClient\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "import json"
      ],
      "metadata": {
        "id": "gQ6bXjfhISK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Key Variables\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/resume/config.json', 'r') as keys:\n",
        "    secret_keys = json.load(keys)\n",
        "\n",
        "db_name = \"human-resource-rag\"\n",
        "collection_name = \"job-applicants-gpt\""
      ],
      "metadata": {
        "id": "-nfZ1NfhRfDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = MongoClient(secret_keys['mongodb_server_connection_url'])\n",
        "collection = client[db_name][collection_name]"
      ],
      "metadata": {
        "id": "BCWZF3_4RxkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load PDF files & Create Embeddings & Load Data to Database\n",
        "loader = DirectoryLoader( '/content', glob=\"./*.pdf\", show_progress=True)\n",
        "data = loader.load()\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(api_key=secret_keys['openai_api_key'],organization = secret_keys['openai_api_org'])\n",
        "vectorStore = MongoDBAtlasVectorSearch.from_documents( data, embeddings, collection=collection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "731cpmTrI6_t",
        "outputId": "b37e14f3-2117-424b-d71a-d8a573ed0440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:34<00:00,  1.72s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional (limit network access to DB)\n",
        "\n",
        "Visit https://cloud.mongodb.com/v2/{SESSION_ID}/security/network/accessList and add your IP address."
      ],
      "metadata": {
        "id": "D0gLvnhw1vCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add current IP Address to MongoDB Whitelist\n",
        "%%time\n",
        "import socket\n",
        "\n",
        "# Get the hostname of the machine\n",
        "hostname = socket.gethostname()\n",
        "\n",
        "# Get the IP address associated with the hostname\n",
        "ip_address = socket.gethostbyname(hostname)\n",
        "\n",
        "print(f\"Hostname: {hostname}\")\n",
        "print(f\"IP Address: {ip_address}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zemmMRfa1QOU",
        "outputId": "ce07b2a9-1563-42b5-ef1c-4d62a72373e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hostname: 2a5ac730c315\n",
            "IP Address: 172.28.0.12\n",
            "CPU times: user 1.55 ms, sys: 0 ns, total: 1.55 ms\n",
            "Wall time: 1.57 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Search Database\n",
        "vector_search = MongoDBAtlasVectorSearch.from_connection_string(\n",
        "   secret_keys['mongodb_server_connection_url'],\n",
        "   db_name+\".\"+collection_name,\n",
        "   OpenAIEmbeddings(api_key=secret_keys['openai_api_key'],organization = secret_keys['openai_api_org']),\n",
        "   index_name=\"default\")"
      ],
      "metadata": {
        "id": "58KYYyQV3Ao5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_query(query):\n",
        "    results = vector_search.similarity_search(query=query,k=2)\n",
        "\n",
        "    search_results=[]\n",
        "    if len(results)>0:\n",
        "      for result in results:\n",
        "        search_results.append(dict(result))\n",
        "\n",
        "    return search_results"
      ],
      "metadata": {
        "id": "WLcqr-GNTTsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute First Query\n",
        "query = \"How many candidates have Systems Design experience?\"\n",
        "\n",
        "[i['page_content'] for i in execute_query(query)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRc3TKsPThOi",
        "outputId": "c2572cc9-8c1b-41dd-b2e7-bc537cbbbcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'metadata', 'page_content', 'type'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['development.\\n\\nof System Engineer.\\n\\nImplemented various Machine Learning Algorithms including Logistic Regression, Support Vector Machine, K-• Fold Cross Validation, Random Forest, K-Nearest Neighbor, and Artificial Neural Network to achieve optimal results.\\n\\nAchieved the highest accuracy in predictions by utilizing the Artificial Neural Network Algorithm. •\\n\\nSkills\\n\\nCertificates\\n\\nCertified - Azure AI Fundamentals (AI 900)\\n\\n(01/2023 - Present), Certified from Azure\\n\\nAWS Certified Cloud Practitioner (AWS CCP) (12/2022 - 12/2025), Certified from AWS\\n\\nMicroso Certified - Azure Fundamentals (AZ 900) (11/2022 - Present), Certified from Azure\\n\\nGoogle Cloud Certified - Associate Cloud Engineer (GCP ACE) (10/2022 - 10/2025), Certified from GCP\\n\\nMachine Learning and Statistical Analysis Unit II (03/2020 - Present), Certified from World Quant University (WQU)\\n\\nScientific Computing and Python for Data Science Unit I (12/2019 - Present), Certified from World Quant University (WQU)\\n\\nEducation\\n\\nNoid\\n\\nTechnolog\\n\\nLanguages\\n\\nEnglish | Hindi\\n\\nInterests\\n\\nReading (Journals, Self-help book) | Listening Music | Travelling | Social Activities | Exercise\\n\\nWatching Movie',\n",
              " 'Align-Justify Experience 05/22 - 01/23 Data Scientist\\n\\nTanalink, Malaysia • Engineered heuristic algorithms to monitor and enhance data quality, resulting in improved accuracy of data analysis by more than 20%. • Developed a module to identify and eliminate bad data using OpenCV and image classification, resulting in 87% accuracy. • Designed an algorithm to accurately map vehicles in high-speed motion, increasing the mapping accuracy by more than 90%. • EDA of geospatial and agronomic datasets to assess worker performance, leading to improved productivity by 12%.\\n\\n06/21 - 05/22 Research Intern\\n\\nAirAsia Digital, Malaysia\\n\\nDeveloped program to automate the planning of cargo loads into airplane using metaheuristic algorithm, reducing computation time by 17% • Simulated demand-supply interactions and developed a naive pricing strategy for ride hailing services, leading to an increase in revenue by 3%. • Identified high activity zones using geospatial analysis, which led to 64% customer retention and decline of 2.5% in ride cancellation. Universiti Teknologi PETRONAS, Malaysia\\n\\nThesis: Physics Informed Deep Learning for non-Newtonian Fluids. • Designed CNN based algorithm that significantly improved the accuracy of fluid flow modelling by 11%. • Contributed to the advancement of AI/ML in computational fluid flow thorough publication in leading journals.\\n\\n2016 - 2018 • Aggregated data from various sources to compile drilling KPI reports and highlighted key areas for optimization, reducing overall cost per foot by 10%. • Interpreted and developed analytical tools to monitor trend and pattern for different operational phases which improved data quality by 20%.\\n\\nData Analyst\\n\\nPetrolink Services, India\\n\\nCLIPBOARD Research Projects Indian Theme: Neural Style Transfer\\n\\nGithub\\n\\nAdaptation of style transfer method to create personalized abstract Indian theme art work.\\n\\nNEURAL-SOLVER\\n\\nGithub\\n\\nPython based AI simulator that uses neural network to solve the different mathematical differential equations without using any kind of training data.\\n\\nGRADUATION-CAP Education 2018 - 2023 Ph.D\\n\\nMachine Learning, Fluid Dynamics\\n\\n2013 - 2015 Masters\\n\\nUniversiti Teknologi PETRONAS Malaysia Andhra University\\n\\nPetroleum Engineering\\n\\nIndia 2009 - 2013 Bachelors\\n\\nUniversity\\n\\nof Petroleum & Energy Studies\\n\\nPetroleum Engineering\\n\\nSCROLL Skills\\n\\nApplied Mathematics Algebra • Deep Learning\\n\\nProbability • Linear\\n\\nPython Programming\\n\\nOptimization\\n\\nIndia']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute Second Query\n",
        "new_query = \"List Candidates with Data Science Experience\"\n",
        "\n",
        "context=[i['page_content'] for i in execute_query(new_query)]"
      ],
      "metadata": {
        "id": "4oWOWzHdr6mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Processing Output with OpenAI\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=secret_keys['openai_api_key'],organization = secret_keys['openai_api_org'])\n",
        "\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a useful assistant. Use the assistant's content to answer the user's query. Create a markdown table with columns 'Candidate name', 'top 5 skills', 'current job title','years of experience' and 'document source' (metadata source)\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"{context}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{new_query}\"}],\n",
        "    temperature = 0.2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJGj1eQF7F1u",
        "outputId": "a0e2579d-b7b0-4943-b7c3-8c8c5c25b138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-9mTvuOMkDKAtkPRyqBnKJU3a4Nm1p', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here is a table listing candidates with Data Science experience:\\n\\n| Candidate Name | Top 5 Skills | Current Job Title | Years of Experience | Document Source |\\n|----------------|--------------|-------------------|---------------------|-----------------|\\n| Kunika Bhargav | Python, SQL, Machine Learning, Data Visualization, NLP | Data Scientist at Almabetter | 2+ years | Resume |\\n| Siddhi Shukla  | R, Python, Machine Learning, SQL, Deep Learning | Data Scientist – Tech Lead at Legato Healthcare | 8 years | Resume |\\n\\nBoth candidates have significant experience in Data Science, with Kunika Bhargav having over 2 years of experience and Siddhi Shukla having 8 years of experience in the field.', role='assistant', function_call=None, tool_calls=None))], created=1721341834, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_c4e5b6fa31', usage=CompletionUsage(completion_tokens=152, prompt_tokens=2046, total_tokens=2198))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TABLE: List Candidates with Data Science Experience\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "IfhXY_-z-C_m",
        "outputId": "6d06fe68-4543-4b3e-8efe-4fc5f2e69224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is a table listing candidates with Data Science experience:\n\n| Candidate Name | Top 5 Skills | Current Job Title | Years of Experience | Document Source |\n|----------------|--------------|-------------------|---------------------|-----------------|\n| Kunika Bhargav | Python, SQL, Machine Learning, Data Visualization, NLP | Data Scientist at Almabetter | 2+ years | Resume |\n| Siddhi Shukla  | R, Python, Machine Learning, SQL, Deep Learning | Data Scientist – Tech Lead at Legato Healthcare | 8 years | Resume |\n\nBoth candidates have significant experience in Data Science, with Kunika Bhargav having over 2 years of experience and Siddhi Shukla having 8 years of experience in the field."
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}